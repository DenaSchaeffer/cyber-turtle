# University of Dayton 2020/2021 - Cyber Threat Intelligence Reporting Tool

## Team 8 - Jacob Scheetz, Justen Stall, Beth Hosek, Dena Schaffer

## Instructor: Dr. Phu Phung

#### Project Description: This project will be used by GE Aviation's Cyber Intelligence team to provide a priority to their research of open-source intelligence (OSINT) reports. As a global company doing high-value work, the volume of relevant reports published daily is too much for even a large team of analysts to read and understand quickly.Our tool would help give priority to what reports need to be read first and what can be overlooked, making sure each piece of vital information gets the attention it needs to continuously give GE's Threat Hunting team a grasp on the cyber space that faces them.

## /commonwords.py
```python
commonwords = [ "a", "about", "all", "also", "and", "as", "at", "be", "because", "but", "by", "can", "come", "could", "day", "do", "even", "find", "first", "for", "from", "get", "give", "go", "have", "he", "her", "here", "him", "his", "how", "I", "if", "in", "into", "it", "its", "just", "know", "like", "look", "make", "man", "many", "me", "more", "my", "new", "no", "not", "now", "of", "on", "one", "only", "or", "other", "our", "out", "people", "say", "see", "she", "so", "some", "take", "tell", "than", "that", "the", "their", "them", "then", "there", "these", "they", "thing", "think", "this", "those", "time", "to", "two", "up", "use", "very", "want", "way", "we", "well", "what", "when", "which", "who", "will", "with", "would", "year", "you", "your" ]
```
## /document.py
```python
'''
Written by: Jacob Scheetz, Justen Stall, Dena Schaeffer, Beth Hosek

Description
===================================
This Document class is defined to efficently handle and retrieve information from each document that is read into the program. As well as ease the formation of the tf-idf model created
to calculate the output of the program.

Class Attributes
===================================
name --> The title of a document (i.e. example-file.pdf)
	- type: string
path --> The absolute path to the document (i.e. C:\example\path\to\file.pdf)
	- type: string
wordcount --> a dictionary consisting of the key:pair values of the words in each file and the occurence of each word in that file, respectively. (i.e. "hacker":3)
	- to avoid repetition, the 300 most common english words are stripped from each file to keep the results meaningful
	- type: dictionary
relevance --> a relevance score generated by calculating a tf-idf model, by using the occurence of a word against a wordlist
	- type: float
'''
class Document:
	name: str
	path: str
	wordcount: dict    
	relevance: float

```
## /myparser.py
```python
'''
myparser.py

Written by: Jacob Scheetz, Justen Stall, Dena Schaeffer, Beth Hosek

Description
===================================
The keygen file contains functions to handle parsing of HTML and PDF files

Class Attributes
===================================
dirWordCount --> gives the wordcount for every word in a document
    -type: dict
listOfWordCounts --> lost of all word counts in the directory
    - type: dict  
req --> the url to be stripped of HTML
    - type: string
soup --> html content of the given url
    - type: string
wordcount --> document word count
    - type: dict
directoryWordCount --> gives the wordcount for every word in a document
    - type: dict
'''

# Usage: python3 keyword_generator.py <Directory Containing Files>
from sys import argv, exit
from io import StringIO
from string import punctuation
import re
from collections import Counter
from json import dump, dumps
import time
import os
import requests

# Additional library imports
# PDFMiner3 handles PDF Parsing
# Beatiful Soup 4 handles HTML parsing
# Feedparser handles xml files
from pdfminer3.high_level import extract_text_to_fp
from bs4 import BeautifulSoup
from progress.bar import IncrementalBar
from document import Document
from commonwords import commonwords
import feedparser


# parseUrl
# Returns page name and text
def parseUrl(url):
    req = requests.get(url)
    soup = BeautifulSoup(req.content, 'html.parser')

    name = soup.find('title').get_text()

    text = soup.get_text()

    return name, text


def parsePdf(inputFile):
    with open(inputFile, 'rb') as i:
        text = StringIO()
        extract_text_to_fp(i, text)
        text = text.getvalue()

    return text


def parseHtml(inputFile):
    with open(inputFile.path, 'rb') as i:
        text = ""
        soup = BeautifulSoup(i, 'html.parser')
        text = soup.get_text()

    return text

def parseRss(url):
    rss = feedparser.parse(url)
    
    return rss
    
    

def countWords(text):
    wordCount = Counter()

    # Split document text into list of words
    wordArray = re.split(' |\?|#|,|\n', text)
    wordArray = [word.lower().strip(punctuation)
                 for word in wordArray]  # Lowercase all words, strip punctuation
    # Filter words for common words
    wordArray = list(
        filter(lambda word: word not in commonwords and word != "", wordArray))

    # Update word counts for document
    wordCount.update(word for word in wordArray)

    documentWordCount = dict(wordCount.most_common())

    return documentWordCount


def countDirectoryWords(documents):
    wordCount = Counter()

    for document in documents:
        # Split document text into list of words
        wordArray = re.split(' |\?|#|,|\n', document.text)
        wordArray = [word.lower().strip(punctuation)
                    for word in wordArray]  # Lowercase all words, strip punctuation
        # Filter words for common words
        wordArray = list(
            filter(lambda word: word not in commonwords and word != "", wordArray))

        # Update word counts for document
        wordCount.update(word for word in wordArray)

        dirWordCount = dict(wordCount.most_common())

    return dirWordCount



```
## /relevance.py
```python
"""
Written by: Dena Schaeffer, Beth Hosek, Justen Stall, Jacob Scheetz

Description
=======================================
File is used to calculate TF, IDF, and TF-IDF. This is a statistical measure used to determine how relevant a term is to a document.
"""
import json
import math
import re
from string import punctuation
from commonwords import commonwords
from keywords import keywords

def computeBasicRelevance(documentText):
    # Split document text into list of words
    wordArray = re.split(' |\?|#|,|\n', documentText)

    wordArray = [word.lower().strip(punctuation)
         for word in wordArray]  # Lowercase all words, strip punctuation
    
    # Filter words for common words
    wordArray = list(filter(lambda word: word not in commonwords and word != "", wordArray))

    keywordsHitCount = 0

    for word in keywords:
        wordHits = wordArray.count(word)
        wordHits = wordHits * keywords[word]/100
        keywordsHitCount += wordHits

    relevance = 1000 * (keywordsHitCount / len(wordArray)) 

    return relevance

def computeTF(wordDict, bagOfWords):
    """
    Caluclates Term-Frequency: measures how frequent a term appears in a document. 
    TF(t)= (Number of times term t appears in a document) / (Total number of terms in the document)

    Parameters
    ----------
    wordDict : dict
        The custom wordlist
    bagOfWords : dict
        All words in the document with their frequency - negates common words
    """

    tfDict = {}
    bagOfWordsCount = len(bagOfWords)
    for word, count in wordDict.items():
        tfDict[word] = count / float(bagOfWordsCount)
    return tfDict

def computeIDF(documents, dirWordCount):
    """
    Calculates Inverse Document Frequency: measures the importance of the term using weights 
    IDF(t) = log_e(Total number of documents / Number of documents with term t in it).


    Parameters
    ----------
    documents : list of Document objects
        All documents in the folder
    allWordCount : dict
        All words with their wordcounts - negates common words
    """

    N = len(documents)

    idfDict = dict.fromkeys(dirWordCount.keys(), 0)
    for document in documents:
        for word, val in document.wordCount.items():
            if val > 0:
                idfDict[word] += 1

    for word, val in idfDict.items():
        idfDict[word] = math.log(N / float(val))
    return idfDict
    
def computeTFIDF(tfBagOfWords, idfs):
    """
    Calculates TF-IDF: Statistical measure used to evaluate how important a word is to a document. 

    Parameters
    ----------
    tfBagOfWords : dict
        Result from computeTF()
    idfs : dict
        Result from computeIDF()
    """
    tfidf = {}
    for word, val in tfBagOfWords.items():
        idfValue = idfs.get(word, 0)
        tfidf[word] = val * idfValue
    return tfidf
```
## /ctirt.py

```python
'''
ctirt.py


Written by: Beth Hosek, Dena Schaeffer, Jacob Scheetz, and Justen Stall for GE Aviation in University of Dayton's Computer Science Capstone

Usage
===================================
Usage: python3 ctirt.py --help [for options]

Required Packages: see environment.yml for required packages

Description
===================================
This Main class is defined to efficently get the relevent words to search for, read files that may be relevant, and compare the two and compute a relevancy score

Class Attributes
===================================
newWordList --> The Word list, as a set of key value pairs
	- type: dictionary
documents --> self defined class, see documentation for further explanation in document.py
    -type: document
listOfWordCounts --> list containing the words of each document split on special characters and stripped for common words
    - type: list 
allWordCount --> a count of all the words that were parsed in a file/directory
    - int
idfs --> inverse document frequency, see http://www.tfidf.com/ for further info
    - type: dictionary
tf --> term frequency, see http://www.tfidf.com/ for further info
    - type: dictionary
tfidf --> term frequency - inverse document frequency model, the resulting relevancy score
    - type: dictionary
relevanceScore --> derived from the tf-idf model, a score given to a document based on its similarity to a predefined wordlist and other factors
    - type: float

'''
from json import dump, dumps, load
import os
from sys import argv, exit
import click
import time
from tabulate import tabulate
from keywords import keywords
from progress.bar import IncrementalBar

import myparser as parser
import relevance as relevance
from document import Document

@click.command()
@click.option('-u', '--url', help='A given url to be parsed for relevance')
@click.option('-f', '--file', 'inputFile', help='Single file to be analyzed', type=click.Path(exists=True))
@click.option('-d', '--directory', help='Files to be analyzed')
@click.option('-rss', '--rss', help='RSS feed to be analyzed')
@click.option('-o', '--output', default='assets/output.json', help='Specify name and location of output file. Default is ')
@click.option('-v', '--verbose', count=True, help='Provides detailed information')
@click.option('-dB', '--debug', count=True ,help='print all of the calculated scores for debugging purposes')
@click.option('-r', '--relevance', 'relevanceAlgorithm', default='basic' ,help='select a relevance algorithm')


def main(url, inputFile, directory, rss, output, verbose, debug, relevanceAlgorithm):

    """
    \n
    \b
    \u001b[34;1m
  /$$$$$$  /$$$$$$$$ /$$$$$$ /$$$$$$$  /$$$$$$$$
 /$$__  $$|__  $$__/|_  $$_/| $$__  $$|__  $$__/
| $$  \__/   | $$     | $$  | $$  \ $$   | $$   
| $$         | $$     | $$  | $$$$$$$/   | $$   
| $$         | $$     | $$  | $$__  $$   | $$   
| $$    $$   | $$     | $$  | $$  \ $$   | $$   
|  $$$$$$/   | $$    /$$$$$$| $$  | $$   | $$   
 \______/    |__/   |______/|__/  |__/   |__/   
                                                
   Cyber Threat Intelligence Reporting Tool                                             
    \n
    \033[39m                                           
    """

    if (len(argv) < 2):
        print(
            "Usage: python3 ctirt.py [options] [target files]\n\n Use --> ctirt.py --help for more details..."
        )
        exit(1)

    if (verbose and url) or (url and debug):
        print("URL is mutually exclusive with verbose and debug")
        exit(1)
    
    
    
    # INITIALIZE DOCUMENTS LIST
    documents = []  # list of document objects

    #RSS INPUT

    if rss:
        printLogo()
        print("Parsing", rss)

        feed = parser.parseRss(rss)
        if not verbose:
            # progress bar
            progressBar = IncrementalBar('Parsing URL', max=len(feed.entries), suffix='%(index)d / %(max)d')

        for entry in feed.entries:
            document = Document()

            document.path = entry.link
            
            document.name, document.text = parser.parseUrl(document.path)
            
            document.wordCount = parser.countWords(document.text)
                    
            # Add document object to list, add document wordcount to list
            documents.append(document)

            if not verbose:
                progressBar.next()
            else:
                print("Done.")
        
        if not verbose:
            progressBar.finish()

        print("Done.")
    # URL INPUT
    
    if url:
        printLogo()
        print("Parsing...")

        document = Document()

        document.path = url
        
        document.name, document.text = parser.parseUrl(url)
        
        document.wordCount = parser.countWords(document.text)
                
        # Add document object to list, add document wordcount to list
        documents.append(document)

        print("Done.")

    
    # SINGLE FILE INPUT

    elif inputFile:
        printLogo()
        print("Parsing...")

        document = Document()

        document.name = os.path.splitext(inputFile)[0]
        document.path = inputFile

        if inputFile.lower().endswith(".pdf"):  # PDF Parsing
            document.text = parser.parsePdf(inputFile)
        elif inputFile.lower().endswith(".html"):  # HTML Parsing
            document.text = parser.parseHtml(inputFile)

        document.wordCount = parser.countWords(document.text)  # Document word count

        # Add document object to list, add document wordcount to list
        documents.append(document)

        print("Done.")


    # DIRECTORY INPUT

    elif directory:
        printLogo()
        if not verbose:
            # progress bar
            progressBar = IncrementalBar('Parsing', max=len(
                os.listdir(directory)), suffix='%(index)d / %(max)d')

        # Loop through files in directory
        for inputFile in os.scandir(directory):
            beginningTime = time.time()

            if verbose:
                timeStamp = time.time()
                print("***[" + inputFile.name[0:50] + "]***", "is currently being parsed",
                    "-->", (timeStamp - beginningTime), "seconds have elapsed...")

            document = Document()

            document.name = os.path.splitext(inputFile.name)[0]
            document.path = inputFile.path

            if verbose:
                print(inputFile.name)

            if inputFile.name.lower().endswith(".pdf"):  # PDF Parsing
                document.text = parser.parsePdf(inputFile.path)
            elif inputFile.name.lower().endswith(".html"):  # HTML Parsing
                document.text = parser.parseHtml(inputFile.path)

            document.wordCount = parser.countWords(
                document.text)  # Document word count

            # Add document object to list, add document wordcount to list
            documents.append(document)

            if not verbose:
                progressBar.next()
            else:
                print("Done.")
        
        if not verbose:
            progressBar.finish()


    # BASIC RELEVANCE CALCULATION

    for document in documents:
        document.relevance = relevance.computeBasicRelevance(document.text)


    # TF-IDF RELEVANCE CALCULATION

    if directory and (verbose or debug or relevanceAlgorithm == "tfidf"):
        dirWordCount = parser.countDirectoryWords(documents)

        wordList = {}
        with open('./assets/wordlist.json') as f:
            jsonWordList = load(f)
            for pair in jsonWordList.items():
                wordList[pair[0]] = float(pair[1])

        for document in documents:
            # TODO Figure out how to run - fix arguments (ex. import wordlist), make debug work better by allowing it to work not in verbose
            idfs = relevance.computeIDF(documents, dirWordCount)
            print("**************** IDFS ****************")
            print(idfs)
            tf = relevance.computeTF(wordList, document.wordCount)
            print("**************** TF DICT ****************")
            print(tf)

            tfidf = relevance.computeTFIDF(tf, idfs)
            print("**************** TF-IDF Values ****************")
            print(tfidf)

            relevanceScore = 0

            for word, val in tfidf.items():
                relevanceScore += val
            
            document.tfidf = relevanceScore * 100


    # OUTPUT SECTION

    documents.sort(key=lambda document: document.relevance, reverse=True)

    table = []
    # print("**************** RELEVANCE SCORES ****************")
    if not verbose:
        for document in documents:
            table.append([document.name[0:70], document.relevance])
            # print(document.name)
            # print(document.relevance)
        print(tabulate(table, headers=["Document","Relevance Score"], tablefmt="fancy_grid"))      
    elif verbose and directory:
        for document in documents:
            table.append([document.name[0:70], document.relevance, document.tfidf, list(document.wordCount.items())[:10]])
            # print(document.name)
            # print(document.relevance)
        print(tabulate(table, headers=["Document","Relevance Score", "TF-IDF Score", "Top Terms"], tablefmt="fancy_grid"))
    else:
        for document in documents:
            table.append([document.name[0:70], document.relevance, list(document.wordCount.items())[:10]])
            # print(document.name)
            # print(document.relevance)
        print(tabulate(table, headers=["Document","Relevance Score", "Top Terms"], tablefmt="fancy_grid"))
    

    # OUTPUT TO FILE

    with open(output, 'w', encoding='utf-8') as o:
        dump([document.__dict__ for document in documents], o)


class color:
   CYAN = '\u001b[36m'
   BASE = '\033[0m'

def printLogo():
    print("""
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@                  @@@@@@@@@@@@@@@@
@@@@@@@@@@@     ((((((     (((((       @@@@@@@@@@
@@@@@@@@    (((((((((((  ((((((((((((     @@@@@@@
@@@@@@    ((((((((((((((((((((((((((((((    @@@@@
@@@@    (((((((((((    ((((((((    ((((((((  @@@@
@@@    ((((((  ((  ((  ((((((  ((  (((((((((   @@
@@    ((((((( ((  ((  (  (((  (   (((((((((((   @
@    (((((((       ((  ((((   (((((((((((((((   @
@      ((((((((((      (   ((      (((((((  (   @
@       (((((((((((    (((   ((((((((((((((     @
@   (((((((((((   (((  ((   ((   (   (((((((    @
@@  (((((((((  (((((  (((   ((  ((((  ((((((    @
@@   (((((((  (((((  (((((  ((((((((  (((((    @@
@@@   ((((((        (((((((         ((((((    @@@
@@@@@   (((((((((((((((((((((((((((((((((    @@@@
@@@@@@@    ((((((((((((((((((((((((((((    @@@@@@
@@@@@@@@@       ((((((((    ((((((((     @@@@@@@@
@@@@@@@@@@@@@                        @@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    GE Aviation - Cyber Threat Reporting Tool 
""".replace('(', color.CYAN + '(' + color.BASE))



if __name__ == "__main__":
    main()


```